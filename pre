import pandas as pd
import numpy as np
from surprise import SVD, Dataset, Reader
from surprise.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score
import os
import difflib
import time
import gc


# Define the dataset folder (please change path)
dataset_folder = r'C:\Users\Ethan\OneDrive\Desktop\ai\SteamGame'


# Check if files exist in the folder
files_to_check = ['recommendations.csv', 'games.csv']
for file in files_to_check:
    file_path = os.path.join(dataset_folder, file)
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Missing file: {file_path}. Ensure the dataset is unzipped into '{dataset_folder}' and files are named correctly. Download from https://www.kaggle.com/datasets/antonkozyriev/game-recommendations-on-steam.")


# Load games data first
games = pd.read_csv(os.path.join(dataset_folder, 'games.csv'))


# Helper: robust title matching (case-insensitive + fuzzy + substring)
def match_title_get_app_id(query_title: str, titles_series: pd.Series, cutoff: float = 0.6):
    import re
    
    # Normalize: lowercase + remove punctuation
    def normalize(text):
        text = str(text).strip().lower()
        # Remove common punctuation but keep spaces
        text = re.sub(r'[:\-™®©\'"\.,!?]', '', text)
        # Normalize multiple spaces to single space
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    # Check if title is non-game content
    def is_non_game_content(title):
        title_upper = str(title).upper()
        non_game_keywords = ['DLC', 'SOUNDTRACK', 'OST', 'MUSIC', 'SOUND TRACK', 
                            'CONTENT PACK', 'COSMETIC', 'SEASON PASS', 'ARTBOOK', 
                            'ART BOOK', 'DIGITAL ART', 'WALLPAPER', 'AVATAR']
        return any(keyword in title_upper for keyword in non_game_keywords)
    
    q = normalize(query_title)
    titles_lower = titles_series.fillna('').astype(str).str.strip().str.lower()
    titles_normalized = titles_lower.apply(normalize)

    # Exact normalized match first - prioritize games over non-game content
    exact_matches = titles_normalized[titles_normalized == q].index
    for idx in exact_matches:
        title = games.loc[idx, 'title']
        if not is_non_game_content(title):
            return games.loc[idx, 'app_id'], title
    # If only non-game content matches, return first
    if len(exact_matches) > 0:
        return games.loc[exact_matches[0], 'app_id'], games.loc[exact_matches[0], 'title']

    # Substring match on normalized - prioritize games
    contains_matches = titles_normalized[titles_normalized.str.contains(re.escape(q), regex=True, na=False)].index
    for idx in contains_matches:
        title = games.loc[idx, 'title']
        if not is_non_game_content(title):
            return games.loc[idx, 'app_id'], title
    # If only non-game content matches, return first
    if len(contains_matches) > 0:
        return games.loc[contains_matches[0], 'app_id'], games.loc[contains_matches[0], 'title']

    # Fuzzy match on normalized titles - prioritize games
    matches = difflib.get_close_matches(q, titles_normalized.tolist(), n=5, cutoff=cutoff)
    for matched_normalized in matches:
        idx = titles_normalized[titles_normalized == matched_normalized].index
        if len(idx) > 0:
            title = games.loc[idx[0], 'title']
            if not is_non_game_content(title):
                return games.loc[idx[0], 'app_id'], title
    # If only non-game content matches, return first
    if matches:
        matched_normalized = matches[0]
        idx = titles_normalized[titles_normalized == matched_normalized].index
        if len(idx) > 0:
            return games.loc[idx[0], 'app_id'], games.loc[idx[0], 'title']
    return None, None


# Get sample size from user
print("\n" + "="*60)
print("Sample Size Configuration")
print("="*60)
print("WARNING: Running large sample sizes may crash your system!")
print("Optimal size: 100,000 - 5,000,000 rows")
print("="*60)


while True:
    try:
        sample_size_input = input("\nEnter sample size (e.g., 100000, 500000, 1000000): ").strip()
        sample_size = int(sample_size_input.replace(",", ""))  # Allow comma-separated input
       
        if sample_size < 10000:
            print("Sample size too small. Minimum is 10,000.")
            continue
        elif sample_size > 10000000:
            print(f"WARNING: You entered {sample_size:,} rows. This is very large and may crash your system!")
            confirm = input("Are you sure you want to continue? (y/n): ").strip().lower()
            if confirm != 'y':
                continue
        elif sample_size > 5000000:
            print(f"You entered {sample_size:,} rows. This is larger than the optimal range.")
            confirm = input("Are you sure you want to continue? (y/n): ").strip().lower()
            if confirm != 'y':
                continue
       
        print(f"\nUsing sample size: {sample_size:,} rows")
        confirm = input("Confirm? (y/n): ").strip().lower()
        if confirm == 'y':
            break
        else:
            continue
    except ValueError:
        print("Invalid input. Please enter a number.")
        continue


# Get DLC filter preference from user
print("\n" + "="*60)
print("Content Filter Configuration")
print("="*60)
while True:
    dlc_filter_input = input("Do you want to exclude DLC, soundtracks, and non-game content? (y/n): ").strip().lower()
    if dlc_filter_input in ['y', 'yes']:
        exclude_dlc = True
        print("Non-game content (DLC, soundtracks, etc.) will be excluded from recommendations.")
        break
    elif dlc_filter_input in ['n', 'no']:
        exclude_dlc = False
        print("All content types will be included in recommendations.")
        break
    else:
        print("Invalid input. Please enter 'y' or 'n'.")


# Load recommendations data efficiently with chunking
print("\nLoading recommendations data...")
file_path = os.path.join(dataset_folder, 'recommendations.csv')
chunk_size = 100000
chunks = []
rows_loaded = 0
load_start = time.time()


# Load chunks until we reach the user-specified sample size
for chunk in pd.read_csv(file_path, chunksize=chunk_size, low_memory=False):
    chunks.append(chunk)
    rows_loaded += len(chunk)
    print(f"  Loaded {rows_loaded:,} rows...")
    if rows_loaded >= sample_size:
        break


print(f"Combining {len(chunks)} chunks...")
all_recommendations = pd.concat(chunks, ignore_index=True)
load_time = time.time() - load_start
print(f"Loaded {len(all_recommendations):,} rows in {load_time:.2f}s")


# Use user-specified sample size
recommendations = all_recommendations.head(sample_size)
del all_recommendations, chunks
gc.collect()
print(f"Using {len(recommendations):,} recommendations for training")

# Validate minimum data requirements
if len(recommendations) < 1000:
    raise ValueError(f"Insufficient data loaded. Only {len(recommendations):,} rows available. Please increase sample size or check data file.")

unique_users = recommendations['user_id'].nunique()
unique_games = recommendations['app_id'].nunique()
print(f"Dataset contains {unique_users:,} unique users and {unique_games:,} unique games")

if unique_games < 10:
    raise ValueError(f"Insufficient game variety. Only {unique_games} unique games found. Please increase sample size.")


# Derive ratings using BOTH is_recommended AND hours played
# This combines direct user feedback with engagement level
print("\nCalculating ratings...")


# Calculate game-specific average hours (to compare relative playtime)
game_avg_hours = recommendations.groupby('app_id')['hours'].transform('mean')
recommendations['relative_hours'] = recommendations['hours'] / (game_avg_hours + 1)  # +1 to avoid division by zero


# Calculate percentiles for relative hours
percentiles = recommendations['relative_hours'].quantile([0.33, 0.66])


def calculate_rating(row):
    """
    Rating system combining is_recommended + relative playtime:
    - is_recommended=True: Base rating 3-5 (depending on playtime)
    - is_recommended=False: Base rating 1-3 (depending on playtime)
    """
    is_rec = str(row['is_recommended']).lower() == 'true'
    rel_hours = row['relative_hours']
   
    if is_rec:
        # User recommended: rating 3, 4, or 5 based on relative playtime
        if rel_hours <= percentiles[0.33]:
            return 3  # Recommended but played less than average
        elif rel_hours <= percentiles[0.66]:
            return 4  # Recommended with average playtime
        else:
            return 5  # Recommended with high playtime
    else:
        # User did NOT recommend: rating 1, 2, or 3 based on relative playtime
        if rel_hours <= percentiles[0.33]:
            return 1  # Not recommended, low playtime
        elif rel_hours <= percentiles[0.66]:
            return 2  # Not recommended, average playtime
        else:
            return 3  # Not recommended but played a lot (gave it a chance)


print("Calculating ratings based on is_recommended + relative playtime...")
recommendations['rating'] = recommendations.apply(calculate_rating, axis=1)
print(f"Rating distribution:\n{recommendations['rating'].value_counts().sort_index()}")


# Normalize ratings per user (remove user bias)
print("\nNormalizing user ratings (removing bias)...")
user_means = recommendations.groupby('user_id')['rating'].transform('mean')
recommendations['normalized_rating'] = recommendations['rating'] - user_means + 3.0  # Normalize to 3.0 mean
print("Rating normalization complete")


# Prepare for Surprise using normalized ratings
print("\nPreparing dataset for training...")
ratings = recommendations[['user_id', 'app_id', 'normalized_rating']].copy()
ratings.columns = ['user_id', 'app_id', 'rating']  # Rename for Surprise
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(ratings, reader)


# Split data
print("Splitting into train/test (80/20)...")
trainset, testset = train_test_split(data, test_size=0.2, random_state=42)


# Train SVD model with optimized parameters
print("\nTraining SVD model (this may take a minute)...")
train_start = time.time()
model = SVD(n_factors=100, n_epochs=30, lr_all=0.01, reg_all=0.1, random_state=42)
model.fit(trainset)
train_time = time.time() - train_start
print(f"Model trained in {train_time:.2f}s")


# Predict and evaluate
print("\nEvaluating model...")
preds = [model.predict(u, i).est for u, i, r in testset]
actuals = [r for u, i, r in testset]
rmse = np.sqrt(np.mean((np.array(preds) - np.array(actuals)) ** 2))
mae = np.mean(np.abs(np.array(preds) - np.array(actuals)))


# Convert predictions and actuals to binary classification (rating >= 3 = positive/liked)
classification_threshold = 3
pred_binary = [1 if p >= classification_threshold else 0 for p in preds]
actual_binary = [1 if a >= classification_threshold else 0 for a in actuals]


# Calculate classification metrics
precision = precision_score(actual_binary, pred_binary, zero_division=0)
recall = recall_score(actual_binary, pred_binary, zero_division=0)
f1 = f1_score(actual_binary, pred_binary, zero_division=0)


print(f'Regression Metrics:')
print(f'  RMSE: {rmse:.4f}')
print(f'  MAE: {mae:.4f}')
print(f'\nClassification Metrics (threshold >= {classification_threshold}):')
print(f'  Precision: {precision:.4f} ({precision*100:.2f}%)')
print(f'  Recall: {recall:.4f} ({recall*100:.2f}%)')
print(f'  F1 Score: {f1:.4f} ({f1*100:.2f}%)')
print(f'\n✓ Model is ready for recommendations!')


# New function: Recommend similar games based on user's favorite game (using collaborative model with a proxy user)
def get_collaborative_recommendations(app_id, top_n=5, exclude_dlc=False):
    try:
        # Find users who have played this game
        users_who_played = recommendations[recommendations['app_id'] == app_id]['user_id'].unique()
        if len(users_who_played) == 0:
            # Game not found in dataset - return None to indicate no data
            return None
       
        # Item-based collaborative filtering: Find games commonly played by users who played this game
        # Get all games played by users who also played the input game
        other_games = recommendations[recommendations['user_id'].isin(users_who_played)]
       
        # Exclude the input game itself
        other_games = other_games[other_games['app_id'] != app_id]
       
        if len(other_games) == 0:
            return []  # No other games found
       
        # Count how many users who played the input game also played each other game
        # Weight by rating (higher rated = better recommendation)
        game_scores = other_games.groupby('app_id').agg({
            'user_id': 'count',  # Number of co-plays
            'rating': 'mean'     # Average rating
        }).reset_index()
        game_scores.columns = ['app_id', 'co_play_count', 'avg_rating']
       
        # Combined score: co-play count * average rating
        game_scores['score'] = game_scores['co_play_count'] * game_scores['avg_rating']
       
        if len(game_scores) == 0:
            return []  # No scores computed
       
        # Sort by score and get top N
        top_games = game_scores.nlargest(min(top_n * 3, len(game_scores)), 'score')  # Get more than needed in case we need to filter DLC
       
        # Get titles and filter DLC if requested
        rec_titles = []
        for _, row in top_games.iterrows():
            game_id = row['app_id']
            title_match = games[games['app_id'] == game_id]['title']
            if not title_match.empty:
                title = title_match.values[0]
                # Filter out DLC and non-game content if requested
                if exclude_dlc:
                    title_upper = title.upper()
                    non_game_keywords = ['DLC', 'SOUNDTRACK', 'OST', 'MUSIC', 'SOUND TRACK', 
                                        'CONTENT PACK', 'COSMETIC', 'SEASON PASS', 'ARTBOOK', 
                                        'ART BOOK', 'DIGITAL ART', 'WALLPAPER']
                    if any(keyword in title_upper for keyword in non_game_keywords):
                        continue
                rec_titles.append(title)
                if len(rec_titles) >= top_n:
                    break
        return rec_titles
    except Exception as e:
        print(f"Error generating recommendations: {str(e)}")
        return []


# Interactive loop for recommendations
print("\n" + "="*60)
print("Game Recommendation System")
print("="*60)
if exclude_dlc:
    print("Content Filter: ON (Non-game content will be excluded from recommendations)")
else:
    print("Content Filter: OFF (All content types will be included)")
print("Type 'quit' or 'exit' to stop the program\n")


while True:
    favorite_game = input("Enter a game you like (or 'quit' to exit): ").strip()
   
    # Check if user wants to exit
    if favorite_game.lower() in ['quit', 'exit', 'q']:
        print("\nThank you for using the Game Recommendation System. Goodbye!")
        break
   
    # Skip empty input
    if not favorite_game:
        print("Please enter a game name.\n")
        continue
   
    # Find closest matching title (robust match)
    app_id, matched_title = match_title_get_app_id(favorite_game, games['title'], cutoff=0.6)
    if app_id is not None:
        print(f"Found match: {matched_title} (app_id={app_id})")
        recommendations_list = get_collaborative_recommendations(app_id, top_n=5, exclude_dlc=exclude_dlc)
        if recommendations_list is None:
            # Game found in games.csv but not in recommendations dataset
            print(f"Sorry, '{matched_title}' was not found in the current sampling size. Try increasing the sample size or try another game.")
        elif recommendations_list:
            print("Here are 5 recommended games based on the collaborative model:")
            for rec in recommendations_list:
                print(f"- {rec}")
        else:
            print(f"Sorry, '{matched_title}' does not have enough data in the current sampling size. Try increasing the sample size or try another game.")
    else:
        print(f"Sorry, '{favorite_game}' was not found in the dataset. Please try another game.")
   
    print()  # Add blank line for readability
